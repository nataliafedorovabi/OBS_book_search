import asyncio
import logging
from typing import Dict, List, Any
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes, Application
from src.vector_store import VectorStore, set_admin_notify_callback, get_voyage_limiter
from src.llm import LLMClient
from src.rate_limiter import RateLimiter
from src.config import ADMIN_TELEGRAM_IDS

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –≤ main.py)
vector_store: VectorStore = None
llm_client: LLMClient = None
rate_limiter: RateLimiter = None
_bot_app: Application = None  # –î–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π

# –•—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∫–Ω–æ–ø–æ–∫ (user_id -> context)
_search_context: Dict[int, Dict[str, Any]] = {}


def init_services(vs: VectorStore, llm: LLMClient, app: Application = None):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤."""
    global vector_store, llm_client, rate_limiter, _bot_app
    vector_store = vs
    llm_client = llm
    rate_limiter = RateLimiter()
    _bot_app = app

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º callback –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –∞–¥–º–∏–Ω–∞ –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å Voyage AI
    set_admin_notify_callback(_send_admin_notification)
    logger.info("Callback –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –∞–¥–º–∏–Ω–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")


def _send_admin_notification(message: str):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –∞–¥–º–∏–Ω–∞–º."""
    if not ADMIN_TELEGRAM_IDS or not _bot_app:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ: {message}")
        return

    async def _send():
        for admin_id in ADMIN_TELEGRAM_IDS:
            try:
                await _bot_app.bot.send_message(
                    chat_id=admin_id,
                    text=f"üö® {message}"
                )
                logger.info(f"–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –∞–¥–º–∏–Ω—É {admin_id}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∞–¥–º–∏–Ω—É {admin_id}: {e}")

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ—Ç–ø—Ä–∞–≤–∫—É
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            asyncio.create_task(_send())
        else:
            loop.run_until_complete(_send())
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")


async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start."""
    docs_count = vector_store.get_count() if vector_store else 0

    await update.message.reply_text(
        f"–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫—É—Ä—Å–∞ '–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–µ–π –∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–æ–º'.\n\n"
        f"–í –±–∞–∑–µ: {docs_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–Ω–∏–≥ –∫—É—Ä—Å–∞.\n\n"
        f"–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º –∫—É—Ä—Å–∞, –∏ —è –Ω–∞–π–¥—É –æ—Ç–≤–µ—Ç –≤ –∫–Ω–∏–≥–∞—Ö.\n\n"
        f"–ö–æ–º–∞–Ω–¥—ã:\n"
        f"/help - –∫–∞–∫ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã\n"
        f"/status - —Å—Ç–∞—Ç—É—Å –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"
    )


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help."""
    await update.message.reply_text(
        "–ö–∞–∫ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã:\n\n"
        "- –§–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ: '–ö–∞–∫–∏–µ —Ä–æ–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –º–µ–Ω–µ–¥–∂–µ—Ä?'\n"
        "- –ú–æ–∂–Ω–æ —Å–ø—Ä–∞—à–∏–≤–∞—Ç—å –ø—Ä–æ —Ç–µ—Ä–º–∏–Ω—ã: '–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ?'\n"
        "- –ú–æ–∂–Ω–æ –ø—Ä–æ—Å–∏—Ç—å —Å—Ä–∞–≤–Ω–∏—Ç—å: '–í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ª–∏–¥–µ—Ä–æ–º –∏ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º?'\n\n"
        "–Ø –æ—Ç–≤–µ—á–∞—é —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–Ω–∏–≥ –∫—É—Ä—Å–∞. "
        "–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö, —è —á–µ—Å—Ç–Ω–æ –æ–± —ç—Ç–æ–º —Å–∫–∞–∂—É."
    )


async def status_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /status."""
    docs_count = vector_store.get_count() if vector_store else 0

    if docs_count > 0:
        await update.message.reply_text(
            f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∞–∫—Ç–∏–≤–Ω–∞.\n"
            f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {docs_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–Ω–∏–≥ –∫—É—Ä—Å–∞."
        )
    else:
        await update.message.reply_text(
            "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."
        )


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π - –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    question = update.message.text.strip()

    if not question:
        return

    if not vector_store or vector_store.get_count() == 0:
        await update.message.reply_text(
            "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        )
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤
    if not rate_limiter.can_make_request():
        await update.message.reply_text(
            "–î–æ—Å—Ç–∏–≥–Ω—É—Ç –¥–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–≤—Ç—Ä–∞.\n"
            "–ü—Ä–∏–Ω–æ—Å–∏–º –∏–∑–≤–∏–Ω–µ–Ω–∏—è –∑–∞ –Ω–µ—É–¥–æ–±—Å—Ç–≤–∞."
        )
        return

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å "–ø–µ—á–∞—Ç–∞–µ—Ç"
    await context.bot.send_chat_action(
        chat_id=update.effective_chat.id,
        action="typing"
    )

    # 1. –°–Ω–∞—á–∞–ª–∞ –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
    relevant_chunks = vector_store.search(question)
    is_expanded = False

    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
    if relevant_chunks:
        top_scores = [f"{c.get('score', 0):.2f}" for c in relevant_chunks[:3]]
        logger.info(f"–ü–µ—Ä–≤—ã–π –ø–æ–∏—Å–∫: {len(relevant_chunks)} —á–∞–Ω–∫–æ–≤, scores={top_scores}")
    else:
        logger.info("–ü–µ—Ä–≤—ã–π –ø–æ–∏—Å–∫: –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    # –£—Å–ª–æ–≤–∏–µ: –≤—ã—Å–æ–∫–∏–π score –ò –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –µ—Å—Ç—å –≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö
    def check_keyword_match(query: str, chunks: list) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—Ç –ª–∏ —á–∞–Ω–∫–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞."""
        import re
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ (>4 –±—É–∫–≤, –Ω–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞)
        stop = {'–Ω–∞–π–¥–∏', '–ø–æ–∫–∞–∂–∏', '—Ä–∞—Å—Å–∫–∞–∂–∏', '–º–æ–¥–µ–ª—å', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '–∫–æ—Ç–æ—Ä—ã–π'}
        words = re.findall(r'[–∞-—è—ëa-z]{5,}', query.lower())
        keywords = [w for w in words if w not in stop]

        if not keywords:
            return True  # –ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –≤ —á–∞–Ω–∫–∞—Ö
        all_text = ' '.join(c.get('text', '').lower() for c in chunks)
        matches = sum(1 for kw in keywords if kw in all_text)
        match_ratio = matches / len(keywords) if keywords else 0

        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {keywords} -> —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π {matches}/{len(keywords)} ({match_ratio:.0%})")
        return match_ratio > 0.5  # –ë–æ–ª—å—à–µ –ø–æ–ª–æ–≤–∏–Ω—ã —Å–ª–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å

    has_good_score = relevant_chunks and any(c.get('score', 0) >= 0.5 for c in relevant_chunks)
    has_keyword_match = check_keyword_match(question, relevant_chunks) if relevant_chunks else False

    has_good_results = has_good_score and has_keyword_match

    if not has_keyword_match and has_good_score:
        logger.info("Score –≤—ã—Å–æ–∫–∏–π, –Ω–æ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã - —Ñ–æ—Ä—Å–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ")

    if not has_good_results:
        logger.info(f"–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Ä–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å")

        # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ LLM
        expanded = llm_client.expand_query(question)
        search_terms = expanded.get('search_terms', [])
        target_chapters = expanded.get('chapters', [])

        logger.info(f"–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ: –≥–ª–∞–≤—ã={target_chapters}, —Ç–µ—Ä–º–∏–Ω—ã={search_terms}")

        if search_terms and target_chapters:
            # –£–ú–ù–´–ô –ü–û–ò–°–ö: –∏—â–µ–º –≤ –ö–ê–ñ–î–û–ô –≥–ª–∞–≤–µ –æ—Ç–¥–µ–ª—å–Ω–æ
            all_chunks = {}
            chapters_found = set()

            for chapter in target_chapters:
                chapter_chunks = {}
                for term in search_terms:
                    # –ò—â–µ–º —ç—Ç–æ—Ç —Ç–µ—Ä–º–∏–Ω –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≥–ª–∞–≤–µ
                    chunks = vector_store.search(term, n_results=2, chapters=[chapter])
                    for chunk in chunks:
                        chunk_id = chunk.get('metadata', {}).get('id', id(chunk))
                        if chunk_id not in chapter_chunks or chunk['score'] > chapter_chunks[chunk_id]['score']:
                            chapter_chunks[chunk_id] = chunk

                # –ë–µ—Ä—ë–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ —ç—Ç–æ–π –≥–ª–∞–≤—ã
                if chapter_chunks:
                    best_from_chapter = sorted(chapter_chunks.values(), key=lambda x: x['score'], reverse=True)[:2]
                    for chunk in best_from_chapter:
                        chunk_id = chunk.get('metadata', {}).get('id', id(chunk))
                        all_chunks[chunk_id] = chunk
                        chapters_found.add(chapter.split('.')[0] if '.' in chapter else chapter)

            logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∏–∑ –≥–ª–∞–≤: {chapters_found}")

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score, –±–µ—Ä—ë–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            relevant_chunks = sorted(all_chunks.values(), key=lambda x: x['score'], reverse=True)[:6]
            is_expanded = True
            logger.info(f"–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—à—ë–ª {len(relevant_chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(chapters_found)} –≥–ª–∞–≤")

        elif search_terms:
            # Fallback: –µ—Å–ª–∏ –≥–ª–∞–≤ –Ω–µ—Ç, –∏—â–µ–º –ø—Ä–æ—Å—Ç–æ –ø–æ —Ç–µ—Ä–º–∏–Ω–∞–º
            all_chunks = {}
            for term in search_terms:
                chunks = vector_store.search(term, n_results=3)
                for chunk in chunks:
                    chunk_id = chunk.get('metadata', {}).get('id', id(chunk))
                    if chunk_id not in all_chunks or chunk['score'] > all_chunks[chunk_id]['score']:
                        all_chunks[chunk_id] = chunk

            relevant_chunks = sorted(all_chunks.values(), key=lambda x: x['score'], reverse=True)[:5]
            is_expanded = True
            logger.info(f"–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ (–±–µ–∑ –≥–ª–∞–≤) –Ω–∞—à—ë–ª {len(relevant_chunks)} —á–∞–Ω–∫–æ–≤")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
    answer = llm_client.generate_answer(question, relevant_chunks, is_expanded_search=is_expanded)

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å
    rate_limiter.record_request()

    # –£–≤–µ–¥–æ–º–ª—è–µ–º –∞–¥–º–∏–Ω–æ–≤ –µ—Å–ª–∏ –ø—Ä–∏–±–ª–∏–∂–∞–µ–º—Å—è –∫ –ª–∏–º–∏—Ç—É
    if rate_limiter.should_warn_admin() and ADMIN_TELEGRAM_IDS:
        usage = rate_limiter.get_usage_info()
        for admin_id in ADMIN_TELEGRAM_IDS:
            try:
                await context.bot.send_message(
                    chat_id=admin_id,
                    text=f"–í–Ω–∏–º–∞–Ω–∏–µ! –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {usage['percent_used']}% –¥–Ω–µ–≤–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞.\n"
                         f"–ó–∞–ø—Ä–æ—Å–æ–≤: {usage['requests_today']}/{usage['limit']}\n"
                         f"–û—Å—Ç–∞–ª–æ—Å—å: {usage['remaining']}"
                )
            except:
                pass
        rate_limiter.mark_warning_sent()

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –≥–ª–∞–≤–∞–º –¥–ª—è –∫–Ω–æ–ø–æ–∫
    chapters_in_results = {}
    for chunk in relevant_chunks:
        chapter = chunk.get('metadata', {}).get('chapter', '–ë–µ–∑ –≥–ª–∞–≤—ã')
        if chapter not in chapters_in_results:
            chapters_in_results[chapter] = []
        chapters_in_results[chapter].append(chunk)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–∏—Å–∫–∞
    user_id = update.effective_user.id
    _search_context[user_id] = {
        'query': question,
        'chunks': relevant_chunks,
        'chapters': chapters_in_results,
        'is_expanded': is_expanded,
        'search_depth': 1  # –£—Ä–æ–≤–µ–Ω—å –≥–ª—É–±–∏–Ω—ã –ø–æ–∏—Å–∫–∞
    }

    # –°–æ–∑–¥–∞—ë–º –∫–Ω–æ–ø–∫–∏
    keyboard = []

    # –ö–Ω–æ–ø–∫–∏ "–ü–æ–¥—Ä–æ–±–Ω–µ–µ" –¥–ª—è –∫–∞–∂–¥–æ–π –≥–ª–∞–≤—ã (–µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–ª–∞–≤)
    if len(chapters_in_results) > 1:
        for i, chapter in enumerate(list(chapters_in_results.keys())[:3]):
            short_name = chapter.split('. ')[1][:20] + '...' if '. ' in chapter and len(chapter.split('. ')[1]) > 20 else chapter.split('. ')[1] if '. ' in chapter else chapter[:25]
            keyboard.append([InlineKeyboardButton(f"üìñ {short_name}", callback_data=f"chapter_{i}")])

    # –ö–Ω–æ–ø–∫–∞ "–ò—Å–∫–∞–ª –¥—Ä—É–≥–æ–µ"
    keyboard.append([InlineKeyboardButton("üîÑ –ò—Å–∫–∞–ª –¥—Ä—É–≥–æ–µ", callback_data="search_other")])

    reply_markup = InlineKeyboardMarkup(keyboard) if keyboard else None

    await update.message.reply_text(answer, reply_markup=reply_markup)


async def usage_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /usage - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–¥–ª—è –∞–¥–º–∏–Ω–æ–≤)."""
    user_id = str(update.effective_user.id)

    # –ö–æ–º–∞–Ω–¥–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–æ–≤
    if ADMIN_TELEGRAM_IDS and user_id not in ADMIN_TELEGRAM_IDS:
        return

    usage = rate_limiter.get_usage_info()
    voyage_stats = vector_store.get_embedding_stats() if vector_store else {}

    await update.message.reply_text(
        f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ {usage['date']}:\n\n"
        f"–ó–∞–ø—Ä–æ—Å–æ–≤: {usage['requests_today']}/{usage['limit']}\n"
        f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {usage['percent_used']}%\n"
        f"–û—Å—Ç–∞–ª–æ—Å—å: {usage['remaining']}\n\n"
        f"üîç Voyage AI (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏):\n"
        f"–ó–∞–ø—Ä–æ—Å–æ–≤: {voyage_stats.get('request_count', 0)}\n"
        f"–¢–æ–∫–µ–Ω–æ–≤: {voyage_stats.get('total_tokens', 0)}\n"
        f"–ú–æ–¥–µ–ª—å: {voyage_stats.get('model', 'N/A')}"
    )


async def voyage_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /voyage - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Voyage AI (–¥–ª—è –∞–¥–º–∏–Ω–æ–≤)."""
    user_id = str(update.effective_user.id)

    # –ö–æ–º–∞–Ω–¥–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–æ–≤
    if ADMIN_TELEGRAM_IDS and user_id not in ADMIN_TELEGRAM_IDS:
        return

    limiter = get_voyage_limiter()
    if not limiter:
        await update.message.reply_text("Voyage –ª–∏–º–∏—Ç–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return

    stats = limiter.get_stats()

    status_emoji = "üü¢" if stats['percent_used'] < 80 else "üü°" if stats['percent_used'] < 90 else "üî¥"
    blocked_status = "‚õî –ó–ê–ë–õ–û–ö–ò–†–û–í–ê–ù" if stats['limit_reached'] else "‚úÖ –ê–∫—Ç–∏–≤–µ–Ω"

    await update.message.reply_text(
        f"üìä Voyage AI –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n\n"
        f"–°—Ç–∞—Ç—É—Å: {blocked_status}\n"
        f"{status_emoji} –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {stats['percent_used']:.2f}%\n\n"
        f"–¢–æ–∫–µ–Ω–æ–≤: {stats['total_tokens']:,} / {stats['free_limit']:,}\n"
        f"–ñ—ë—Å—Ç–∫–∏–π –ª–∏–º–∏—Ç: {stats['hard_limit']:,}\n"
        f"–û—Å—Ç–∞–ª–æ—Å—å –¥–æ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏: {stats['remaining']:,}\n\n"
        f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {'–î–∞' if stats['warning_sent'] else '–ù–µ—Ç'}\n\n"
        f"–°–±—Ä–æ—Å–∏—Ç—å —Å—á—ë—Ç—á–∏–∫: /voyage_reset"
    )


async def voyage_reset_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /voyage_reset - —Å–±—Ä–æ—Å —Å—á—ë—Ç—á–∏–∫–∞ (–¥–ª—è –∞–¥–º–∏–Ω–æ–≤)."""
    user_id = str(update.effective_user.id)

    # –ö–æ–º–∞–Ω–¥–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–¥–º–∏–Ω–æ–≤
    if ADMIN_TELEGRAM_IDS and user_id not in ADMIN_TELEGRAM_IDS:
        return

    limiter = get_voyage_limiter()
    if not limiter:
        await update.message.reply_text("Voyage –ª–∏–º–∏—Ç–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
    args = context.args
    if args and args[0] == "CONFIRM":
        stats_before = limiter.get_stats()
        if limiter.reset(admin_confirmed=True):
            await update.message.reply_text(
                f"‚úÖ –°—á—ë—Ç—á–∏–∫ Voyage AI —Å–±—Ä–æ—à–µ–Ω!\n\n"
                f"–ë—ã–ª–æ: {stats_before['total_tokens']:,} —Ç–æ–∫–µ–Ω–æ–≤\n"
                f"–°—Ç–∞–ª–æ: 0 —Ç–æ–∫–µ–Ω–æ–≤\n\n"
                f"–ë–æ—Ç —Å–Ω–æ–≤–∞ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã."
            )
            logger.info(f"Voyage —Å—á—ë—Ç—á–∏–∫ —Å–±—Ä–æ—à–µ–Ω –∞–¥–º–∏–Ω–æ–º {user_id}")
        else:
            await update.message.reply_text("‚ùå –û—à–∏–±–∫–∞ —Å–±—Ä–æ—Å–∞ —Å—á—ë—Ç—á–∏–∫–∞.")
    else:
        stats = limiter.get_stats()
        await update.message.reply_text(
            f"‚ö†Ô∏è –í—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ —Å–±—Ä–æ—Å–∏—Ç—å —Å—á—ë—Ç—á–∏–∫?\n\n"
            f"–¢–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {stats['total_tokens']:,} —Ç–æ–∫–µ–Ω–æ–≤\n\n"
            f"–≠—Ç–æ —Å–ª–µ–¥—É–µ—Ç –¥–µ–ª–∞—Ç—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏:\n"
            f"‚Ä¢ –ù–∞—á–∞–ª—Å—è –Ω–æ–≤—ã–π –ø–µ—Ä–∏–æ–¥ (–º–µ—Å—è—Ü)\n"
            f"‚Ä¢ –í—ã –ø–æ–ø–æ–ª–Ω–∏–ª–∏ –±–∞–ª–∞–Ω—Å Voyage AI\n\n"
            f"–î–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: /voyage_reset CONFIRM"
        )


async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–∞–∂–∞—Ç–∏–π –Ω–∞ inline –∫–Ω–æ–ø–∫–∏."""
    query = update.callback_query
    await query.answer()

    user_id = update.effective_user.id
    search_ctx = _search_context.get(user_id)

    if not search_ctx:
        await query.edit_message_reply_markup(reply_markup=None)
        await query.message.reply_text("–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–∏—Å–∫–∞ —É—Å—Ç–∞—Ä–µ–ª. –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –∑–∞–Ω–æ–≤–æ.")
        return

    callback_data = query.data

    if callback_data.startswith("chapter_"):
        # –ù–∞–∂–∞–ª–∏ –Ω–∞ –∫–Ω–æ–ø–∫—É –≥–ª–∞–≤—ã - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω–µ–µ
        chapter_idx = int(callback_data.split("_")[1])
        chapters_list = list(search_ctx['chapters'].keys())

        if chapter_idx >= len(chapters_list):
            await query.message.reply_text("–ì–ª–∞–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
            return

        chapter_name = chapters_list[chapter_idx]
        chapter_chunks = search_ctx['chapters'][chapter_name]

        # –ò—â–µ–º –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —ç—Ç–æ–π –≥–ª–∞–≤–µ
        original_query = search_ctx['query']
        logger.info(f"–£–≥–ª—É–±–ª—ë–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –≥–ª–∞–≤–µ: {chapter_name}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å
        await context.bot.send_chat_action(
            chat_id=update.effective_chat.id,
            action="typing"
        )

        # –ò—â–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ –∏–∑ —ç—Ç–æ–π –≥–ª–∞–≤—ã
        more_chunks = vector_store.search(original_query, n_results=5, chapters=[chapter_name])

        if more_chunks:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ –≥–ª–∞–≤–µ
            detailed_answer = llm_client.generate_answer(
                f"{original_query} (–ø–æ–¥—Ä–æ–±–Ω–µ–µ –∏–∑ –≥–ª–∞–≤—ã '{chapter_name}')",
                more_chunks,
                is_expanded_search=True
            )
            rate_limiter.record_request()

            await query.message.reply_text(
                f"üìñ *–ü–æ–¥—Ä–æ–±–Ω–µ–µ –∏–∑ {chapter_name}:*\n\n{detailed_answer}",
                parse_mode="Markdown"
            )
        else:
            await query.message.reply_text(f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –≥–ª–∞–≤–µ '{chapter_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

    elif callback_data == "search_other":
        # –ù–∞–∂–∞–ª–∏ "–ò—Å–∫–∞–ª –¥—Ä—É–≥–æ–µ" - —É–≥–ª—É–±–ª—è–µ–º –ø–æ–∏—Å–∫
        original_query = search_ctx['query']
        search_depth = search_ctx.get('search_depth', 1)

        if search_depth >= 3:
            await query.message.reply_text(
                "–ü–æ–∏—Å–∫ —É–∂–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–∞—Å—à–∏—Ä–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∑–∞–¥–∞—Ç—å –µ–≥–æ –∏–Ω–∞—á–µ."
            )
            return

        logger.info(f"–†–∞—Å—à–∏—Ä—è–µ–º –ø–æ–∏—Å–∫, –≥–ª—É–±–∏–Ω–∞: {search_depth + 1}")

        await context.bot.send_chat_action(
            chat_id=update.effective_chat.id,
            action="typing"
        )

        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
        expanded = llm_client.expand_query(original_query)
        search_terms = expanded.get('search_terms', [])
        target_chapters = expanded.get('chapters', [])

        # –ò—â–µ–º —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        all_chunks = {}
        for chapter in target_chapters:
            for term in search_terms:
                chunks = vector_store.search(term, n_results=4, chapters=[chapter])
                for chunk in chunks:
                    chunk_id = chunk.get('metadata', {}).get('id', id(chunk))
                    if chunk_id not in all_chunks or chunk['score'] > all_chunks[chunk_id]['score']:
                        all_chunks[chunk_id] = chunk

        # –ò—Å–∫–ª—é—á–∞–µ–º —É–∂–µ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
        shown_ids = {c.get('metadata', {}).get('id') for c in search_ctx['chunks']}
        new_chunks = [c for c in all_chunks.values() if c.get('metadata', {}).get('id') not in shown_ids]
        new_chunks = sorted(new_chunks, key=lambda x: x['score'], reverse=True)[:5]

        if new_chunks:
            answer = llm_client.generate_answer(original_query, new_chunks, is_expanded_search=True)
            rate_limiter.record_request()

            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            _search_context[user_id]['chunks'].extend(new_chunks)
            _search_context[user_id]['search_depth'] = search_depth + 1

            # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –µ—â—ë –æ–¥–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            keyboard = [[InlineKeyboardButton("üîÑ –ï—â—ë –≤–∞—Ä–∏–∞–Ω—Ç—ã", callback_data="search_other")]]
            reply_markup = InlineKeyboardMarkup(keyboard)

            await query.message.reply_text(
                f"üîç *–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:*\n\n{answer}",
                parse_mode="Markdown",
                reply_markup=reply_markup
            )
        else:
            await query.message.reply_text(
                "–î—Ä—É–≥–∏—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
            )
